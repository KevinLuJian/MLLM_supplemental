<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Evaluation Datasets</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f9;
            color: #333;
        }
        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
        }
        header {
            background: #333;
            color: #fff;
            padding-top: 30px;
            min-height: 70px;
            border-bottom: #77aaff 3px solid;
        }
        header a {
            color: #fff;
            text-decoration: none;
            text-transform: uppercase;
            font-size: 16px;
        }
        header ul {
            padding: 0;
            list-style: none;
        }
        header li {
            display: inline;
            padding: 0 20px;
        }
        .main-content {
            margin: 20px 0;
        }
        .stats ul {
            list-style: none;
            padding: 0;
        }
        .stats ul li {
            background: #e9ecef;
            margin-bottom: 5px;
            padding: 10px;
            border: #ccc 1px solid;
            border-radius: 5px;
        }
        footer {
            background: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
            margin-top: 20px;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        a {
            color: #007bff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .authors p {
            font-style: italic;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Revisiting Multi-Model LLM Evaluations</h1>
            <div class="authors">
                <p>Jian Lu, Shikhar Srivastava, Junyu Chen, Robik Shrestha, Manoj Acharya, Kushal Kafle, Christopher Kanan</p>
            </div>
        </div>
    </header>

    <div class="container">
        <div class="main-content">
            <h1>Introduction</h1>
            <p>
              In this paper, we evaluate popular multi-modal large language models (MLLMs) on four datasets created by our lab: VQDv1, TallyQA, TDIUC, and DVQA. These datasets are designed to assess the performance of multi-modal LLMs on various tasks, including visual question answering, counting, and image understanding. 
              VQDv1, TallyQA, and DVQA evaluations are not based on complete datasets but instead use representative samples. This effectively shortens the testing sets, allowing developers to expedite the evaluation process while maintaining the diversity of the original datasets. This websites provides the datasets, evaluation scripts, and example answer files for researchers to reproduce our results.
            </p>
            <h2>Datasets</h2>
            <h3>VQDv1</h3> 
            <p>VQDv1 requires the model to produce multiple bounding boxes instead of localizing only one object, thereby testing general query detection skills. Unlike typical referring expression datasets, which assert that every query will correspond to only one bounding box, VQDv1 queries ask the model to generate an uncertain number of bounding boxes, from 0 to N, posing an additional challenge to the model.
            </p>
            <p>
                To download the question-answer pairs of our sampled version of VQDv1, 
                please go to <a href="https://github.com/KevinLuJian/MLLM-evaluation/raw/main/VQDv1_sampling.json">this link</a>.
            </p>
            <p>
                To download the images, please download val2014 from <a href="http://images.cocodataset.org/zips/val2014.zip">this link</a>.
            </p>

            <h3>TallyQA</h3>
            <p>TallyQA tests models' visual grounding through counting skills. In addition to simple counting questions that the model can handle well with straightforward object detection, TallyQA also incorporates complex counting questions that demand sophisticated reasoning capabilities, such as pose estimation (e.g., "How many dogs are sitting?") and positional reasoning (e.g., "How many dogs are in front of the white building?").
            </p>
            <p> We directly use the complete version of the TallyQA testing dataset, which has a reasonable size. To download the testing dataset of TallyQA, please go to <a href="https://github.com/KevinLuJian/MLLM-evaluation/raw/main/TallyQA_test.json">this link</a>.</p>
            <p> To download the images, please go to <a href="https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip">this link</a> and <a href="https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip">this link</a>.</p>
            
            <h3>TDIUC</h3>
            <p>TDIUC tests the models' versatility across 12 tasks, including object, attribute, and activity recognition, as well as overall scene understanding. The meaningful categories of question types permit fine-grain analysis of the models' abilities from different perspectives, allowing us to identify the specific strengths and weaknesses of each model.
            </p>
            <p>To download the question-answer pairs of our sampled version of TDIUC, please go to <a href="https://github.com/KevinLuJian/MLLM-evaluation/raw/main/TDIUC_sampling.json">this link</a>.</p>
            <p>To download the images, please go to <a href="https://drive.google.com/file/d/1Hevf7eQNzg-qlXbfz9nPbATmQciexkDp/view?usp=share_link">this link</a>.</p>

            <h3>DVQA</h3>
            <p>DVQA requires the models to interpret and analyze visual data in chart form, testing their ability to perform OCR and handle unusual words found in charts. The charts are all synthetically generated images, which pose additional challenges different from natural images.</p>
            <p>To download the question-answer pairs of our sampled version of DVQA, please go to <a href="https://github.com/KevinLuJian/MLLM-evaluation/raw/main/DVQA_sampling.json">this link</a>.</p>
            <p>To download the images, please go to <a href="https://drive.google.com/file/d/1iOSjgbqnTiLpMFuuRa3kIs3E_RxGkKmX/view?usp=share_link">this link</a>.</p>

            <h2>Evaluation Script</h2>
            <p>To evaluate the performance of the models on the datasets, we provide evaluation scripts for each dataset. Please prepare the answer files in the format of the question-answer pairs we provided. You can download the evaluation scripts from this repository: <a href="https://github.com/KevinLuJian/MLLM-evaluation/tree/main/eval_script">script</a>. Detailed instructions on how to use the scripts are available in the repository.</p>

            <h2>Example Answer Files</h2>
            <p>For each dataset, we provide example answer files in the format of the question-answer pairs we used. The example answer files can be found at <a href="https://github.com/KevinLuJian/MLLM-evaluation/tree/main/Evaluation_result(Ours)">this link</a>. The most important components are the predicted answer and the labeled answer.</p>
            <h2>Data Sampling script</h2>
            <p>For the datasets, VQDv1, TDIUC, and DVQA, where we sample a portion of the original testing datsets, we provide the <a href="https://github.com/KevinLuJian/MLLM-evaluation/tree/main/datasets_sampling">sampling scripts</a> that show how the datasets are being sampled, with detailed instructions in readme file.</p>
        </div>
    </div>

    <footer>
        <div class="container">
            <p>&copy; 2024 Multi-Model LLM Evaluations. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
